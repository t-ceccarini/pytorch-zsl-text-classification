{"cells":[{"cell_type":"markdown","metadata":{"id":"0dP87m9aE2iI"},"source":["#Zero Shot Text Classification"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"],"metadata":{"id":"YNHRdeYYlXT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install dependencies and import libraries"],"metadata":{"id":"FP1zwA6HlBWg"}},{"cell_type":"code","source":["#Classification as NLI\n","!pip install transformers --quiet\n","!pip install sentencepiece --quiet\n","!pip install datasets --quiet\n","\n","#LASER Language-Agnostic SEntence Representations\n","#%env LASER=${HOME}/projects/laser\n","#!git clone https://github.com/facebookresearch/LASER.git --quiet\n","#!bash LASER/install_models.sh\n","#!bash LASER/install_external_tools.sh\n","#!pip install faiss\n","#!bash LASER/tasks/xnli/xnli.sh\n","#!git clone https://github.com/facebookresearch/MLDoc.git --quiet\n","#!git clone https://gitlab.mi.hdm-stuttgart.de/griesshaber/nlp-corpora.git --quiet\n","#!pip3 install -r /content/nlp-corpora/requirements.txt\n","#!pip install cucco --quiet\n","#!python nlp-corpora/scripts/rcv_preprocess.py\n","#!python MLDoc/generate_documents.py --indices-file MLDoc/mldoc-indices/french.train.1000 --output-filename MLDoc/rcv2_out/french.train.1000.out --rcv-dir ../../../data/reuters/rcv2/RCV2_Multilingual_Corpus/french/\n","\n","#LASER embeddings\n","!pip install laserembeddings --quiet\n","!pip install laserembeddings[ja] --quiet\n","!python -m laserembeddings download-models\n"],"metadata":{"id":"eitw1elHIdeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Classification as NLI\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix\n","from transformers import pipeline\n","from datasets import load_metric, DatasetDict, dataset_dict\n","import pickle\n","import os\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","from transformers import Trainer, TrainingArguments\n","from transformers import DataCollatorWithPadding\n","import json\n","\n","#LASER embeddings\n","from laserembeddings import Laser\n","from datasets import load_dataset\n","import torch\n","from torch import optim,nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import csv"],"metadata":{"id":"HAG9_nJRlilS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVSsjFhU1aam"},"source":["## Classification as NLI"]},{"cell_type":"markdown","source":["### Methods definition"],"metadata":{"id":"h5EJCCHWAodG"}},{"cell_type":"code","source":["def evaluate_batches(classifier,\n","             batched_sequences,\n","             labels,\n","             hypothesis_template,\n","             end,\n","             multi_label=True,\n","             reset = False):\n","\n","  if reset == True:\n","    results = []\n","    current = 0\n","    os.remove(\"/content/gdrive/MyDrive/batches_written.txt\")\n","  else:\n","    with open(\"/content/gdrive/MyDrive/results.dat\", \"rb\") as file_results:\n","      results = pickle.load(file_results)\n","\n","    with open(\"/content/gdrive/MyDrive/batches_written.txt\", \"rb\") as file_batches_rd:\n","      for line in file_batches_rd:\n","        pass\n","      current = int(line) + 1\n","\n","  batched_sequences = batched_sequences[current:end]\n","\n","  for batch in batched_sequences:\n","    results.append(evaluate(classifier, batch, labels, hypothesis_template, multi_label=True))\n","\n","    with open(\"/content/gdrive/MyDrive/batches_written.txt\", \"a\") as file_batches:\n","      file_batches.write(str(current)+\"\\n\")\n","\n","    with open(\"/content/gdrive/MyDrive/results.dat\", \"wb\") as file_results:\n","      pickle.dump(results, file_results)\n","\n","    print(f\"Batch n. {current} written.\")\n","    current = current + 1\n","\n","  return results\n","\n","def evaluate(classifier,\n","             batch,\n","             labels,\n","             hypothesis_template,\n","             multi_label=True):\n","\n","  result = classifier(sequences = batch,\n","                      candidate_labels = labels,\n","                      hypothesis_template = hypothesis_template,\n","                      multi_label = multi_label)\n","\n","  return result\n","\n","def split_list(list,\n","               batch_size):\n","  chunked_list = [list[i:i+batch_size] for i in range(0, len(list), batch_size)]\n","\n","  return chunked_list\n","\n","def from_label_to_class_index(result, labels):\n","  preds = []\n","  for r in result:\n","    for lab in labels:\n","      if r['labels'][0] == lab:\n","        preds.append(labels.index(lab))\n","\n","  return preds\n","\n","def from_label_to_class_index2(result, lab_to_ind):\n","  preds = []\n","  for r in result:\n","    preds.append(lab_to_ind[r['labels'][0]])\n","\n","  return preds\n","\n","def from_label_to_class_index_harsh_policy(result, lab_to_ind, seen, unseen, alpha=0.05):\n","  preds = []\n","  for r in result:\n","    if r['scores'][0] <= r['scores'][1] + alpha and r['labels'][1] in unseen and r['labels'][0] in seen:\n","      preds.append(lab_to_ind[r['labels'][1]])\n","    else:\n","      preds.append(lab_to_ind[r['labels'][0]])\n","\n","  return preds"],"metadata":{"id":"rWTTgNhfM5qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def define_hyp_from_indices(indices, ind_to_lab, hypothesis_template = \"This example is about \"):\n","  hypothesis = []\n","  for i,index in enumerate(indices):\n","    hypothesis.append(hypothesis_template+ind_to_lab[indices[i]])\n","\n","  return hypothesis\n","\n","def select_subset_by_classes(train_data, labels, classes_train):\n","  subset_train = []\n","  subset_lab = []\n","  og_index = []\n","\n","  for i,(example,lab) in enumerate(zip(train_data,labels)):\n","    if lab in classes_train:\n","      subset_train.append(example)\n","      subset_lab.append(lab)\n","      og_index.append(i)\n","\n","  return subset_train,subset_lab,og_index\n","\n","def make_negative_hypothesis(data, hypothesis, lab_to_ind):\n","  enh_data = data.copy()\n","  enh_hypothesis = hypothesis.copy()\n","  te_labels = [lab_to_ind[\"Positive\"]]*len(data)\n","  unique_hyp = list(set(hypothesis))\n","\n","  for i,(example,hyp) in enumerate(zip(data,hypothesis)):\n","\n","    for un_hyp in unique_hyp:\n","      if un_hyp != hyp:\n","        enh_data.append(example)\n","        enh_hypothesis.append(un_hyp)\n","        te_labels.append(lab_to_ind[\"Negative\"])\n","\n","  return enh_data,enh_hypothesis,te_labels\n","\n","def preprocess_text_partially_unlabeled(premises, labels, ind_to_lab, lab_to_ind_te, classes_selected, partial=True, size=-1):\n","  if partial == True:\n","    premises,labels,og_index = select_subset_by_classes(premises,\n","                                                        labels,\n","                                                        classes_selected)\n","\n","  hypothesis = define_hyp_from_indices(labels,\n","                                       ind_to_lab)\n","\n","  premises,hypothesis,labels = make_negative_hypothesis(premises,\n","                                                        hypothesis,\n","                                                        lab_to_ind_te)\n","\n","  if size != -1:\n","    premises = premises[:size]\n","    hypothesis = hypothesis[:size]\n","    labels = labels[:size]\n","\n","  encodings = tokenizer(premises, hypothesis, return_tensors='pt',\n","                        padding=True,\n","                        truncation='only_first')\n","\n","  return encodings, labels\n",""],"metadata":{"id":"ZhQdV4puPuus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(preds,labels,average=\"weighted\"):\n","\n","  acc_metric = load_metric(\"accuracy\")\n","  f1_metric = load_metric(\"f1\")\n","  precision_metric = load_metric(\"precision\")\n","  recall_metric = load_metric(\"recall\")\n","\n","  if average == None:\n","    accuracy = confusion_matrix(preds,labels,normalize=\"true\").diagonal()\n","  else:\n","    accuracy = acc_metric.compute(predictions=preds,references=labels)[\"accuracy\"]\n","\n","  f1 = f1_metric.compute(predictions=preds,references=labels,average=average)[\"f1\"]\n","  precision = precision_metric.compute(predictions=preds,references=labels,average=average)[\"precision\"]\n","  recall = recall_metric.compute(predictions=preds,references=labels,average=average)[\"recall\"]\n","\n","  if average == None:\n","    accuracy = accuracy.tolist()\n","    f1 = f1.tolist()\n","    precision = precision.tolist()\n","    recall = recall.tolist()\n","\n","  return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"],"metadata":{"id":"HusdFTvvhbsU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_dictionary(file_path,file_name,dictionary):\n","  with open(file_path+'/'+file_name+'.json','w+') as f:\n","    json.dump(dictionary,f)\n","\n","def load_dictionary(full_path):\n","  f = open(full_path)\n","  dictionary = json.load(f)\n","  return dictionary"],"metadata":{"id":"gH79BnIep4Yu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import datasets and preprocessing"],"metadata":{"id":"PYdk4zWNe-D0"}},{"cell_type":"code","source":["#Import datasets\n","\n","\"\"\"AG-NEWS\"\"\"\n","#ag_news_dataset = load_dataset(\"ag_news\")\n","\n","\"\"\"NLU-EVALUATION\"\"\"\n","#nlu_evaluation_dataset = load_dataset(\"nlu_evaluation_data\")\n","#\n","#def assign_new_label(example, list):\n","#  example['new_label'] = list.index(example['scenario'])\n","#  return example\n","#\n","#labels = ['cooking','audio','weather','calendar','music','email','qa','alarm','play','lists',\n","#          'iot','datetime','recommendation','transport','general','takeaway','news','social']\n","#assign_new_label_nlu = lambda example : assign_new_label(example,labels)\n","#nlu_evaluation_dataset = nlu_evaluation_dataset.map(assign_new_label_nlu)\n","#nlu_evaluation_dataset = nlu_evaluation_dataset[\"train\"].train_test_split(test_size=0.1)\n","\n","\"\"\"EMOEVENT\"\"\"\n","access_token = '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'\n","emoevent_dataset = load_dataset(\"fmplaza/EmoEvent\", 'en', use_auth_token=access_token)\n","\n","#test_df_sp = pd.read_csv(r'/content/gdrive/MyDrive/sentipolc_test_set_preprocessed.csv', skiprows=1, error_bad_lines=False, names=[\"idtwitter\",\"subj\",\"opos\",\"oneg\",\"iro\",\"lpos\",\"lneg\",\"top\",\"text\",\"polarity\"])\n","\n","#test_df_ya = pd.read_csv(r'/content/gdrive/MyDrive/test-ya.csv', names=[\"class index\",\"question title\",\"question content\",\"best answers\"])\n","\n","#test_df_ya = test_df_ya.sample(n=2000, random_state=17)"],"metadata":{"id":"FYN0948LfAkd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model definition"],"metadata":{"id":"cAo-ZZc4cl_A"}},{"cell_type":"markdown","source":["**Label-fully-unseen setting:** evaluate a pretrained on a different task without any fine-tuning.\n","\n","**Label-partially-unseen setting:** first, perform fine-tuning using a subset of the labels, then evaluate separately the performance of the model on the two subsets of labels. Alternatively, evaluate an already fine-tuned model."],"metadata":{"id":"eDSXfAeebGNH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2s1txiYvFeQO"},"outputs":[],"source":["#Label-fully-unseen setting\n","\n","#zero_shot_classifier = pipeline(\"zero-shot-classification\",\n","#                                device=0,\n","#                                #model='facebook/bart-large-mnli',                #8-17 minutes on a subset of ya, 0.57 acc, 0.56 f1\n","#                                #model=\"joeddav/xlm-roberta-large-xnli\",           #14 minutes on a subset of ya, 0.49 acc, 0.49 f1\n","#                                #model=\"typeform/distilbert-base-uncased-mnli\"    #1 minutes on a subset of ya, 0.34 acc, 0.35 f1\n","#                                model='cross-encoder/nli-distilroberta-base'     #2 minutes on a subset of ya, 0.44 acc, 0.42 f1\n","#                                )\n","\n","#Label-partially-unseen setting\n","\n","#In this case we will also perform fine-tuning using a subset of the dataset classes\n","#model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n","#tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n","\n","model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-distilroberta-base')\n","tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-distilroberta-base')\n","\n","#Import an already fine-tuned model\n","#zero_shot_classifier = pipeline(\"zero-shot-classification\",\n","#                                device=0,\n","#                                model=\"joeddav/bart-large-mnli-yahoo-answers\"\n","#                                )"]},{"cell_type":"markdown","source":["### Training (label-partially-unseen setting)"],"metadata":{"id":"0w1f9lVsc1eW"}},{"cell_type":"code","source":["# Turn our labels and encodings into a Dataset object\n","\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        #item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n"],"metadata":{"id":"OL3JC8DRKkXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"AG-NEWS\n","train_encodings, train_labels = preprocess_text_partially_unlabeled(premises = ag_news_dataset[\"train\"][\"text\"],\n","                                                      labels = ag_news_dataset[\"train\"][\"label\"],\n","                                                      ind_to_lab = {2:\"Business\",3:\"Sci/Tech\"},\n","                                                      lab_to_ind_te = {\"Negative\":0, \"Positive\":1},\n","                                                      classes_selected = [2,3])\n","\n","test_encodings, test_labels = preprocess_text_partially_unlabeled(premises = ag_news_dataset[\"test\"][\"text\"],\n","                                                     labels = ag_news_dataset[\"test\"][\"label\"],\n","                                                     ind_to_lab = {0:\"World\",1:\"Sports\", 2:\"Business\",3:\"Sci/Tech\"},\n","                                                     lab_to_ind_te = {\"Negative\":0, \"Positive\":1},\n","                                                     classes_selected = [0,1,2,3],\n","                                                     partial = False)\n","\"\"\"\n","\n","\"\"\"NLU-EVALUATION-DATA\n","labels_to_index = {k: v for v, k in enumerate(labels)}\n","index_to_labels_test = dict((v,k) for k,v in labels_to_index.items())\n","index_to_labels_train = dict((k, index_to_labels_test[k]) for k in (0,2,4,6,8,10,12,14,16))\n","\n","train_encodings, train_labels = preprocess_text_partially_unlabeled(premises = nlu_evaluation_dataset['train']['text'],\n","                                                      labels = nlu_evaluation_dataset[\"train\"][\"new_label\"],\n","                                                      ind_to_lab = index_to_labels_train,\n","                                                      lab_to_ind_te = {\"Negative\":0, \"Positive\":1},\n","                                                      classes_selected = [0,2,4,6,8,10,12,14,16])\n","\n","test_encodings, test_labels = preprocess_text_partially_unlabeled(premises = nlu_evaluation_dataset['test']['text'],\n","                                                     labels = nlu_evaluation_dataset[\"test\"][\"new_label\"],\n","                                                     ind_to_lab = index_to_labels_test,\n","                                                     lab_to_ind_te = {\"Negative\":0, \"Positive\":1},\n","                                                     classes_selected = list(range(18)),\n","                                                     partial = False)\n","\"\"\"\n","\n","\"\"\"EMOEVENT\"\"\"\n","train_encodings, train_labels = preprocess_text_partially_unlabeled(premises = emoevent_dataset[\"train\"][\"tweet\"],\n","                                                      labels = emoevent_dataset[\"train\"][\"label\"],\n","                                                      ind_to_lab = {0:'anger',1:'fear',4:'disgust',6:'other'},\n","                                                      lab_to_ind_te = {\"Negative\":0, \"Positive\":1},\n","                                                      classes_selected = [0,1,4,6])\n","\n","test_encodings, test_labels = preprocess_text_partially_unlabeled(premises = emoevent_dataset[\"test\"][\"tweet\"],\n","                                                     labels = emoevent_dataset[\"test\"][\"label\"],\n","                                                     ind_to_lab = {0:'anger',1:'fear',2:'sadness',3:'joy',4:'disgust',5:'surprise',6:'other'},\n","                                                     lab_to_ind_te = {\"Negative\":0, \"Positive\":1},\n","                                                     classes_selected = [0,1,2,3,4,5,6],\n","                                                     partial = False)\n","\n","train_dataset = TextDataset(train_encodings, train_labels)\n","test_dataset = TextDataset(test_encodings, test_labels)"],"metadata":{"id":"GXQsAJMUHmPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine-tuning with Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,             # total number of training epochs\n","    per_device_train_batch_size=32,  # batch size per device during training\n","    per_device_eval_batch_size=32,   # batch size for evaluation\n","    learning_rate=2e-5,             # the initial learning rate for AdamW optimizer\n","    #max_grad_norm=0.01,             # maximum gradient norm (for gradient clipping)\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.1,                # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=25,\n","    save_strategy='epoch',            # save is done at the end of each epoch\n","    evaluation_strategy='epoch',\n","    eval_steps='epoch',              # evaluation is done at the end of each epoch\n","    #load_best_model_at_end=True      # whether or not to load the best model found during training at the end of training\n",")\n","\n","trainer = Trainer(\n","    model=model.cuda(),                  # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=test_dataset            # evaluation dataset\n",")"],"metadata":{"id":"aSyc4R9XOiYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip \"/content/gdrive/MyDrive/results_nlu.zip\" -d \"/content/gdrive/MyDrive/results_nlu\""],"metadata":{"id":"erT4T41noxde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start (or resume) training for fine-tuning with Trainer\n","#trainer.train(\"/content/gdrive/MyDrive/results3/content/results/checkpoint-11250\")\n","#trainer.train(\"/content/gdrive/MyDrive/results_nlu/content/results/checkpoint-8106\")\n","trainer.train()"],"metadata":{"id":"uLQ5I-kpOm1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the model\n","#torch.save(model, '/content/gdrive/MyDrive/models/best_nli-distilroberta-base_nlu.pt')\n","torch.save(model, '/content/gdrive/MyDrive/models/best_nli-distilroberta-base_ee.pt')"],"metadata":{"id":"1s5hkll7FdHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Zip and store results\n","!zip -r /content/results_ee10.zip /content/results\n","!cp -r '/content/results_ee10.zip' /content/gdrive/MyDrive/"],"metadata":{"id":"iNOO2mN4kKxp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!unzip \"/content/gdrive/MyDrive/results_nlu4.zip\" -d \"/content/gdrive/MyDrive/results_nlu4\"\n","!unzip \"/content/gdrive/MyDrive/results_ee4.zip\" -d \"/content/gdrive/MyDrive/results_ee4\""],"metadata":{"id":"mk_vxPDs9oCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Inference (label-fully-unseen and partially-unseen setting)"],"metadata":{"id":"mrtVw1H-dCzb"}},{"cell_type":"code","source":["#Load the model\n","#model = torch.load('/content/gdrive/MyDrive/models/best_nli-distilroberta-base_ag.pt')\n","model = torch.load('/content/gdrive/MyDrive/models/best_nli-distilroberta-base_nlu.pt')\n","model.eval()\n","tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-distilroberta-base')"],"metadata":{"id":"HG83S-gxG9-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-distilroberta-base')\n","tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-distilroberta-base')\n","#model.load_state_dict(torch.load('/content/gdrive/MyDrive/results4/content/results/checkpoint-15000/pytorch_model.bin'))\n","#model.load_state_dict(torch.load('/content/gdrive/MyDrive/results_nlu4/content/results/checkpoint-16252/pytorch_model.bin'))\n","model.load_state_dict(torch.load('/content/gdrive/MyDrive/results_ee4/content/results/checkpoint-1616/pytorch_model.bin'))\n","\n","model.eval()"],"metadata":{"id":"qumMQEzidrKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()"],"metadata":{"id":"zTaQnWeOd5cC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["zero_shot_classifier = pipeline(\"zero-shot-classification\",\n","                                device=0,\n","                                model=model,\n","                                tokenizer=tokenizer)"],"metadata":{"id":"LWR7UaPSp9u8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUveaWIk0RQR"},"outputs":[],"source":["\"\"\" INFERENCE ON SENTIPOLC\n","sequences_sp = test_df_sp[\"text\"].tolist()\n","hypothesis_template = \"This text is about {}.\"\n","labels_sp = [\"Positive\", \"Negative\", \"Mixed\", \"Neutral\"]\n","\n","hypothesis_template = \"Questo esempio Ã¨ {}.\"\n","labels_sp = [\"positivo\", \"negativo\", \"misto\", \"neutrale\"]\n","result_sp = zero_shot_classifier(sequences_sp,\n","                     labels_sp,\n","                     hypothesis_template=hypothesis_template,\n","                     multi_label=False)\n","\"\"\"\n","\n","\"\"\"INFERENCE ON YAHOO ANSWERS\n","sequences_ya = test_df_ya[\"question title\"].tolist()\n","hypothesis_template = \"This text is about {}.\"\n","labels_ya = [\"Society & Culture\",\n","             \"Science & Mathematics\",\n","             \"Health\",\"Education & Reference\",\n","             \"Computers & Internet\",\n","             \"Sports\",\n","             \"Business & Finance\",\n","             \"Entertainment & Music\",\n","             \"Family & Relationships\",\n","             \"Politics & Government\"]\n","\n","result_ya = evaluate(zero_shot_classifier,\n","                     sequences_ya,\n","                     labels_ya,\n","                     hypothesis_template=hypothesis_template,\n","                     multi_label=True)\n","\"\"\"\n","\n","\"\"\" INFERENCE ON AG_NEWS\n","sequences_ag = ag_news_dataset[\"test\"][\"text\"]\n","polarities_ag = ag_news_dataset[\"test\"][\"label\"]\n","\n","hypothesis_template = \"This text is about {}.\"\n","labels_ag = [\"World\",\n","             \"Sports\",\n","             \"Business\",\n","             \"Sci/Tech\"]\n","\n","result_ag = evaluate(zero_shot_classifier,\n","                     sequences_ag,\n","                     labels_ag,\n","                     hypothesis_template=hypothesis_template,\n","                     multi_label=False)\n","\"\"\"\n","\n","\"\"\" INFERENCE ON NLU_EVALUATION_DATA\n","sequences = nlu_evaluation_dataset['test']['text']\n","polarities = nlu_evaluation_dataset[\"test\"][\"new_label\"]\n","\n","hypothesis_template = \"This text is about {}.\"\n","#labels = list(set(nlu_evaluation_dataset['train']['scenario']))\n","labels = ['cooking','audio','weather','calendar','music','email','qa','alarm','play','lists',\n","          'iot','datetime','recommendation','transport','general','takeaway','news','social']\n","\n","result = evaluate(zero_shot_classifier,\n","                  sequences,\n","                  labels,\n","                  hypothesis_template=hypothesis_template,\n","                  multi_label=False)\n","\"\"\"\n","\n","\"\"\" INFERENCE ON EMOEVENT\"\"\"\n","sequences = emoevent_dataset['test']['tweet']\n","polarities = emoevent_dataset['test']['label']\n","\n","hypothesis_template = \"This text is about {}.\"\n","labels = ['anger','fear','sadness','joy','disgust','surprise','other']\n","\n","result = evaluate(zero_shot_classifier,\n","                  sequences,\n","                  labels,\n","                  hypothesis_template=hypothesis_template,\n","                  multi_label=False)"]},{"cell_type":"code","source":["result"],"metadata":{"id":"XT942dZsV_g1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emoevent_dataset['test']['tweet']"],"metadata":{"id":"BuG_UM7HT2yo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"id":"YCB28SY4c59T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save_dictionary('/content/gdrive/MyDrive/inference-results','results_ag_pu_distilroberta_base_ml_false_23',result_ag)\n","#save_dictionary('/content/gdrive/MyDrive/inference-results','results_nlu_fu_distilroberta_base_ml_false',result)\n","#save_dictionary('/content/gdrive/MyDrive/inference-results','results_nlu_pu_distilroberta_base_ml_false',result)\n","save_dictionary('/content/gdrive/MyDrive/inference-results','results_ee_fu_distilroberta_base_ml_false',result)"],"metadata":{"id":"K8kKe4_ziMTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#result = load_dictionary('/content/gdrive/MyDrive/inference-results/results_nlu_fu_distilroberta_base_ml_false.json')\n","#result = load_dictionary('/content/gdrive/MyDrive/inference-results/results_nlu_pu_distilroberta_base_ml_false.json')\n","result = load_dictionary('/content/gdrive/MyDrive/inference-results/results_ee_fu_distilroberta_base_ml_false.json')"],"metadata":{"id":"xlDeh21QMdXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePW3Oj6w1LjJ"},"outputs":[],"source":["#preds_sp = from_label_to_class_index(result_sp, labels_sp)\n","#polarities_sp = test_df_sp[\"polarity\"].tolist()\n","\n","#preds_ya = from_label_to_class_index(result_ya, labels_ya)\n","#polarities_ya = (test_df_ya[\"class index\"] - 1).tolist()   #we need to subtract 1 because we want class index between 0 and 9\n","\n","#preds_ag = from_label_to_class_index2(result_ag,{\"World\":0,\"Sports\":1,\"Business\":2,\"Sci/Tech\":3})\n","#preds_ag = from_label_to_class_index_harsh_policy(result_ag,{\"World\":0,\"Sports\":1,\"Business\":2,\"Sci/Tech\":3},[\"Business\",\"Sci/Tech\"],[\"World\",\"Sports\"])\n","#metrics = compute_metrics(preds_ag,polarities_ag)\n","\n","#polarities = nlu_evaluation_dataset[\"test\"][\"new_label\"]\n","#labels = ['cooking','audio','weather','calendar','music','email','qa','alarm','play','lists',\n","#          'iot','datetime','recommendation','transport','general','takeaway','news','social']\n","#labels_to_index = {k: v for v, k in enumerate(labels)}\n","#preds_nlu = from_label_to_class_index2(result,labels_to_index)\n","#metrics = compute_metrics(preds_nlu,polarities)\n","#metrics_class_by_class = compute_metrics(preds_nlu,polarities,average=None)\n","\n","polarities = emoevent_dataset['test']['label']\n","labels = ['anger','fear','sadness','joy','disgust','surprise','other']\n","labels_to_index = {k: v for v, k in enumerate(labels)}\n","preds_ee = from_label_to_class_index2(result,labels_to_index)\n","metrics = compute_metrics(preds_ee,polarities)\n","metrics_class_by_class = compute_metrics(preds_ee,polarities,average=None)\n","\n"]},{"cell_type":"code","source":["metrics"],"metadata":{"id":"AbRlkmkLaAWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_class_by_class"],"metadata":{"id":"zR7rKNBdN6U4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save_dictionary('/content/gdrive/MyDrive/metrics','metrics_ag_pu_distilroberta_base_ml_false_23',metrics)\n","\n","#save_dictionary('/content/gdrive/MyDrive/metrics','metrics_nlu_fu_distilroberta_base_ml_false',metrics)\n","#save_dictionary('/content/gdrive/MyDrive/metrics','metrics_nlu_pu_distilroberta_base_ml_false',metrics)\n","#save_dictionary('/content/gdrive/MyDrive/metrics','metrics_nlu_pu_distilroberta_base_ml_false_classes',metrics_class_by_class)\n","\n","save_dictionary('/content/gdrive/MyDrive/metrics','metrics_ee_fu_distilroberta_base_ml_false',metrics)\n","save_dictionary('/content/gdrive/MyDrive/metrics','metrics_ee_fu_distilroberta_base_ml_false_classes',metrics_class_by_class)\n"],"metadata":{"id":"tBJWY94Aq_al"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"YoIsNcs5uiDg"}},{"cell_type":"code","source":["metrics_ag_pu = load_dictionary('/content/gdrive/MyDrive/metrics/metrics_ag_pu_distilroberta_base_ml_false_23.json')\n","metrics_ag_fu = load_dictionary('/content/gdrive/MyDrive/metrics/metrics_ag_fu_distilroberta_base.json')\n","\n","metrics_nlu_pu = load_dictionary('/content/gdrive/MyDrive/metrics/metrics_nlu_pu_distilroberta_base_ml_false.json')\n","metrics_nlu_fu = load_dictionary('/content/gdrive/MyDrive/metrics/metrics_nlu_fu_distilroberta_base_ml_false.json')\n","metrics_nlu_pu_class_by_class = load_dictionary('/content/gdrive/MyDrive/metrics/metrics_nlu_pu_distilroberta_base_ml_false_classes.json')\n","\n","metrics_ee_fu = load_dictionary('/content/gdrive/MyDrive/metrics/metrics_ee_fu_distilroberta_base_ml_false.json')"],"metadata":{"id":"6P6Vy29MtM3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compare_partially_fully_unseen(datasets):\n","  cols = []\n","  for dataset in datasets:\n","    cols.append((dataset,'FU'))\n","    cols.append((dataset,'PU'))\n","  df = pd.DataFrame(columns = cols)\n","  df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Dataset','Model'])\n","\n","  return df\n","\n","def append_metric(df,model_name,dataset_name,mode,value):\n","  df.loc[model_name,(dataset_name,mode)] = value\n","\n","  return df\n","\n","def compare_perfomance_on_labels(labels,seen,metrics,order):\n","  metrics_names = list(metrics.keys())\n","\n","  ord_metrics = metrics.copy()\n","  for metric in metrics_names:\n","    ord_metrics[metric] = [ord_metrics[metric][i] for i in order]\n","\n","  ord_labels = [labels[i] for i in order]\n","\n","  #for metric in metrics_names:\n","  #  cols.append(metric)\n","\n","  cols = pd.MultiIndex.from_tuples([('Metrics',metric) for metric in metrics_names])\n","  idx = pd.MultiIndex.from_tuples([(i,j) for i,j in zip(seen,ord_labels)])\n","  df = pd.DataFrame(index=idx,columns = cols)\n","\n","  for (s,l) in zip(seen,ord_labels):\n","    index = ord_labels.index(l)\n","    for metric in metrics_names:\n","      val = ord_metrics[metric][index]\n","      df.loc[(s,l),('Metrics',metric)] = val\n","\n","  return df\n",""],"metadata":{"id":"O1dDgSgwPY3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_par_vs_ful = compare_partially_fully_unseen(['ag_news','nlu_evaluation_data','EmoEvent'])\n","append_metric(df_par_vs_ful,'nli-distilroberta-base','ag_news','PU',metrics_ag_pu['accuracy'])\n","append_metric(df_par_vs_ful,'nli-distilroberta-base','ag_news','FU',metrics_ag_fu['accuracy'])\n","append_metric(df_par_vs_ful,'nli-distilroberta-base','nlu_evaluation_data','PU',metrics_nlu_pu['accuracy'])\n","append_metric(df_par_vs_ful,'nli-distilroberta-base','nlu_evaluation_data','FU',metrics_nlu_fu['accuracy'])\n","append_metric(df_par_vs_ful,'nli-distilroberta-base','EmoEvent','FU',metrics_ee_fu['accuracy'])\n","df_par_vs_ful"],"metadata":{"id":"PHolqvDTGFan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"AG_NEWS\"\"\"\n","#labs = [\"World\",\"Sports\",\"Business\",\"Sci/Tech\"]\n","#seen = ['Unseen','Unseen','Seen','Seen']\n","\n","\"\"\"NLU_EVALUATION_DATA\"\"\"\n","labs = ['cooking','audio','weather','calendar','music','email','qa','alarm','play','lists',\n","          'iot','datetime','recommendation','transport','general','takeaway','news','social']\n","#labs_ord = [*[labs[i] for i in range(0,18,2)],*[labs[i] for i in range(1,18,2)]]\n","order = [*[i for i in range(0,18,2)],*[i for i in range(1,18,2)]]\n","sorted_labs = [labs[i] for i in order]\n","seen = [*['Seen']*9,*['Unseen']*9]\n","\n","\"\"\"EMOEVENT\"\"\"\n","\n"],"metadata":{"id":"hSjAa6iNcZrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_labels = compare_perfomance_on_labels(labs,seen,metrics_nlu_pu_class_by_class,order)\n","df_labels"],"metadata":{"id":"dQNwj8XrfF0l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LASER embeddings"],"metadata":{"id":"C2FmktP78Gar"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","\n","  def __init__(self,\n","               idim = 4*1024,\n","               odim = 3,\n","               nhid = [512, 384],\n","               activation = 'relu',\n","               batch_norm = False,\n","               dropout = 0.3,\n","               dropout_first = True,\n","               device = 'cuda'):\n","\n","        super(MLP, self).__init__()\n","\n","        modules = []\n","\n","        if dropout > 0 and dropout_first == True:\n","          modules.append(nn.Dropout(p=dropout))\n","\n","        nprev = idim\n","        for nh in nhid:\n","\n","          modules.append(nn.Linear(nprev,nh))\n","          nprev = nh\n","\n","          if activation == 'relu':\n","            modules.append(nn.ReLU())\n","          elif activation == 'tanh':\n","            modules.append(nn.Tanh())\n","          else:\n","            raise Exception(\"Unrecognized activation function\")\n","\n","          if batch_norm == True:\n","            modules.append(nn.BatchNorm1d(num_features=nh))\n","\n","          if dropout > 0:\n","            modules.append(nn.Dropout(p=dropout))\n","\n","        modules.append(nn.Linear(nh,odim))\n","        modules.append(nn.Softmax(dim=1))\n","\n","        self.mlp = nn.Sequential(*modules)\n","        if device == 'cuda':\n","          self.mlp = self.mlp.to(device)\n","\n","  def forward(self, x):\n","    return self.mlp(x)\n"],"metadata":{"id":"wA20TnUw7ZRN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### LASER embeddings + XNLI MLP Training/Inference"],"metadata":{"id":"rQe_2xU7FqjV"}},{"cell_type":"markdown","source":["##### LASER embeddings + XNLI MLP Classes and Methods\n","\n","\n"],"metadata":{"id":"E6cZh8fPi4Sc"}},{"cell_type":"code","source":["# Turn our labels and encodings into a Dataset object\n","\n","class XnliDataset(torch.utils.data.Dataset):\n","    def __init__(self, premises, hypothesis, labels):\n","        self.premises = premises\n","        self.hypothesis = hypothesis\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        #item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item = {}\n","        item['premises'] = torch.tensor(self.premises[idx], dtype=torch.float32)\n","        item['hypothesis'] = torch.tensor(self.hypothesis[idx], dtype=torch.float32)\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n"],"metadata":{"id":"B5e5m0dfUn_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model,\n","          optimizer,\n","          criterion,\n","          trainloader,\n","          validloader,\n","          device='cuda',\n","          epochs=5):\n","\n","  best_valid_loss = float('Inf')\n","  model.train()\n","\n","  for epoch in range(epochs):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","\n","        inputs_prem = data['premises'].to('cuda')\n","        inputs_hyp = data['hypothesis'].to('cuda')\n","        inputs = prepare_input_xnli(inputs_prem,inputs_hyp)\n","        labels = data['labels'].to('cuda')\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:\n","            model.eval()\n","\n","            with torch.no_grad():\n","              valid_loss = 0.0\n","              nb_batch_eval = 0\n","              for batch_eval in validloader:\n","                inputs_prem = batch_eval['premises'].to('cuda')\n","                inputs_hyp = batch_eval['hypothesis'].to('cuda')\n","                inputs = prepare_input_xnli(inputs_prem,inputs_hyp)\n","                labels = batch_eval['labels'].to('cuda')\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                valid_loss += loss.item()\n","                nb_batch_eval = nb_batch_eval + 1\n","\n","            print(f'[{epoch + 1}, {i + 1:5d}] train loss: {running_loss / 200:.3f}, valid loss: {valid_loss / nb_batch_eval:.3f}')\n","\n","            if best_valid_loss > valid_loss:\n","              best_valid_loss = valid_loss\n","              torch.save(model,'/content/gdrive/MyDrive/models/best_xnli.pt')\n","\n","            running_loss = 0.0\n","\n","            model.train()\n","\n","\n","  print('Finished Training')\n"],"metadata":{"id":"OdJ2i_8fRdR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def embed_xnli_data(data, laser, lang):\n","  premise = data['premise']\n","  hyp = data['hypothesis']\n","  labels = data['label']\n","\n","  emb_premise = laser.embed_sentences(premise,lang=lang)\n","  emb_hyp = laser.embed_sentences(hyp,lang=lang)\n","\n","  return emb_premise,emb_hyp,labels"],"metadata":{"id":"CiJMjQdOwt65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_input_xnli(premise,hyp):\n","  input = torch.cat([premise,hyp,premise*hyp,torch.abs(premise-hyp)],dim=1)\n","\n","  return input\n"],"metadata":{"id":"6Lxnx9tmVX-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy_xnli(model, dataloader):\n","  n_correct = 0\n","  tot = 0\n","  model.eval()\n","\n","  for batch in dataloader:\n","    inputs_prem = batch['premises'].to('cuda')\n","    inputs_hyp = batch['hypothesis'].to('cuda')\n","    inputs = prepare_input_xnli(inputs_prem,inputs_hyp)\n","    labels = batch['labels'].to('cuda')\n","\n","    outputs = model(inputs)\n","    preds = torch.argmax(outputs, dim=1)\n","\n","    check = preds == labels\n","\n","    n_correct = n_correct + torch.sum(check)\n","    tot = tot + labels.shape[0]\n","\n","  acc = n_correct/tot\n","\n","  return acc\n"],"metadata":{"id":"puDaVQuZG0zs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LASER embeddings + XNLI MLP Training"],"metadata":{"id":"2RQHV3SEe3I_"}},{"cell_type":"code","source":["# Set GPU and random seed\n","np.random.seed(159753)\n","torch.manual_seed(159753)\n","torch.cuda.manual_seed(159753)"],"metadata":{"id":"H7BonxI2MLI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["laser = Laser()"],"metadata":{"id":"tVqNc-pVYknD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp_xnli = MLP(dropout_first=False)"],"metadata":{"id":"cZvymy1dJIcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZ_vzHLYxl9D"},"outputs":[],"source":["xnli_en = load_dataset(\"xnli\",\"en\")\n","xnli_es = load_dataset(\"xnli\",\"es\")"]},{"cell_type":"code","source":["#xnli_eng_train_prem, xnli_eng_train_hyp, xnli_eng_train_lab = embed_xnli_data(xnli_en['train'], laser, lang='en')\n","#xnli_esp_train_prem, xnli_esp_train_hyp, xnli_esp_train_lab = embed_xnli_data(xnli_es['validation'], laser, lang='es')\n","\n","xnli_eng_train_prem = np.loadtxt('/content/gdrive/MyDrive/Data/xnli_eng_train_prem.csv', delimiter=',', dtype=np.float32)\n","xnli_eng_train_hyp = np.loadtxt('/content/gdrive/MyDrive/Data/xnli_eng_train_hyp.csv', delimiter=',', dtype=np.float32)\n","xnli_eng_train_lab = np.loadtxt('/content/gdrive/MyDrive/Data/xnli_eng_train_lab.csv', delimiter=',', dtype=np.float32)\n","\n","xnli_esp_train_prem = np.loadtxt('/content/gdrive/MyDrive/Data/xnli_esp_train_prem.csv', delimiter=',', dtype=np.float32)\n","xnli_esp_train_hyp = np.loadtxt('/content/gdrive/MyDrive/Data/xnli_esp_train_hyp.csv', delimiter=',', dtype=np.float32)\n","xnli_esp_train_lab = np.loadtxt('/content/gdrive/MyDrive/Data/xnli_esp_train_lab.csv', delimiter=',', dtype=np.float32)"],"metadata":{"id":"b4CnJsxPxSPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#np.savetxt('/content/gdrive/MyDrive/Data/xnli_eng_train_prem.csv', xnli_eng_train_prem, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/xnli_eng_train_hyp.csv', xnli_eng_train_hyp, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/xnli_eng_train_lab.csv', xnli_eng_train_lab, delimiter=',')\n","#\n","#np.savetxt('/content/gdrive/MyDrive/Data/xnli_esp_train_prem.csv', xnli_esp_train_prem, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/xnli_esp_train_hyp.csv', xnli_esp_train_hyp, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/xnli_esp_train_lab.csv', xnli_esp_train_lab, delimiter=',')"],"metadata":{"id":"Kfh1WVNBVnP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = XnliDataset(xnli_eng_train_prem, xnli_eng_train_hyp, xnli_eng_train_lab)\n","valid_dataset = XnliDataset(xnli_esp_train_prem, xnli_esp_train_hyp, xnli_esp_train_lab)"],"metadata":{"id":"8fqJHnBSudQA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainloader = DataLoader(train_dataset,batch_size=128,shuffle=True)\n","validloader = DataLoader(valid_dataset,batch_size=128,shuffle=True)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(mlp_xnli.parameters(),lr=1e-4)\n","\n","train(model=mlp_xnli,\n","      optimizer=optimizer,\n","      criterion = criterion,\n","      trainloader = trainloader,\n","      validloader = validloader,\n","      epochs=5)"],"metadata":{"id":"tF9Ph6c2vB0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save model\n","torch.save(mlp_xnli,'/content/gdrive/MyDrive/models/last_xnli.pt')"],"metadata":{"id":"gtFpFX3agT8v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LASER embeddings + XNLI MLP Inference"],"metadata":{"id":"bxVBTQKTe9nh"}},{"cell_type":"code","source":["laser = Laser()"],"metadata":{"id":"W8bvf-cPfITe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load model\n","mlp_xnli = MLP()\n","mlp_xnli = torch.load('/content/gdrive/MyDrive/models/best_xnli.pt')"],"metadata":{"id":"_b8VlZAPfITf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xnli_en = load_dataset(\"xnli\",\"en\")\n","xnli_fr = load_dataset(\"xnli\",\"fr\")\n","xnli_es = load_dataset(\"xnli\",\"es\")\n","xnli_de = load_dataset(\"xnli\",\"de\")\n","xnli_ar = load_dataset(\"xnli\",\"ar\")\n","xnli_zh = load_dataset(\"xnli\",\"zh\")\n","xnli_ur = load_dataset(\"xnli\",\"ur\")"],"metadata":{"id":"ZcYNiZTmfITg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xnli_en_test_prem, xnli_en_test_hyp, xnli_en_test_lab = embed_xnli_data(xnli_en['test'], laser, lang='en')\n","xnli_fr_test_prem, xnli_fr_test_hyp, xnli_fr_test_lab = embed_xnli_data(xnli_fr['test'], laser, lang='fr')\n","xnli_esp_test_prem, xnli_esp_test_hyp, xnli_esp_test_lab = embed_xnli_data(xnli_es['test'], laser, lang='es')\n","xnli_de_test_prem, xnli_de_test_hyp, xnli_de_test_lab = embed_xnli_data(xnli_de['test'], laser, lang='de')\n","xnli_ar_test_prem, xnli_ar_test_hyp, xnli_ar_test_lab = embed_xnli_data(xnli_ar['test'], laser, lang='ar')\n","xnli_zh_test_prem, xnli_zh_test_hyp, xnli_zh_test_lab = embed_xnli_data(xnli_zh['test'], laser, lang='zh')\n","xnli_ur_test_prem, xnli_ur_test_hyp, xnli_ur_test_lab = embed_xnli_data(xnli_ur['test'], laser, lang='ur')"],"metadata":{"id":"hlRcXftEfITg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset_en = XnliDataset(xnli_en_test_prem, xnli_en_test_hyp, xnli_en_test_lab)\n","test_dataset_fr = XnliDataset(xnli_fr_test_prem, xnli_fr_test_hyp, xnli_fr_test_lab)\n","test_dataset_esp = XnliDataset(xnli_esp_test_prem, xnli_esp_test_hyp, xnli_esp_test_lab)\n","test_dataset_de = XnliDataset(xnli_de_test_prem, xnli_de_test_hyp, xnli_de_test_lab)\n","test_dataset_ar = XnliDataset(xnli_ar_test_prem, xnli_ar_test_hyp, xnli_ar_test_lab)\n","test_dataset_zh = XnliDataset(xnli_zh_test_prem, xnli_zh_test_hyp, xnli_zh_test_lab)\n","test_dataset_ur = XnliDataset(xnli_ur_test_prem, xnli_ur_test_hyp, xnli_ur_test_lab)"],"metadata":{"id":"_8w_pte6fITh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testloader_en = DataLoader(test_dataset_en,batch_size=128,shuffle=True)\n","testloader_fr = DataLoader(test_dataset_fr,batch_size=128,shuffle=True)\n","testloader_es = DataLoader(test_dataset_esp,batch_size=128,shuffle=True)\n","testloader_de = DataLoader(test_dataset_de,batch_size=128,shuffle=True)\n","testloader_ar = DataLoader(test_dataset_ar,batch_size=128,shuffle=True)\n","testloader_zh = DataLoader(test_dataset_zh,batch_size=128,shuffle=True)\n","testloader_ur = DataLoader(test_dataset_ur,batch_size=128,shuffle=True)"],"metadata":{"id":"8e6YCHS2fITh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["compute_accs = lambda dl : compute_accuracy_xnli(mlp_xnli,dl)"],"metadata":{"id":"Yd7u1_5ufITi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dls = [testloader_en,\n","       testloader_fr,\n","       testloader_es,\n","       testloader_de,\n","       testloader_ar,\n","       testloader_zh,\n","       testloader_ur]\n","\n","accs = map(compute_accs,dls)"],"metadata":{"id":"Ewww8GlEfITi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(accs)"],"metadata":{"id":"ldKWKjrnfITi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_sp = pd.read_csv(r'/content/gdrive/MyDrive/sentipolc_test_set_preprocessed.csv', skiprows=1, names=[\"idtwitter\",\"subj\",\"opos\",\"oneg\",\"iro\",\"lpos\",\"lneg\",\"top\",\"text\",\"polarity\"])\n","\n","premises = test_df_sp['text'].tolist()\n","#hypothesis = ['This example is positive.','This example is negative.','This example is mixed.','This example is neutral.']\n","hypothesis = ['Questo esempio Ã¨ positivo.','Questo esempio Ã¨ negativo.','Questo esempio Ã¨ misto.','Questo esempio Ã¨ neutrale.']\n","pols = test_df_sp['polarity'].tolist()\n","\n","emb_prem = torch.tensor(laser.embed_sentences(premises,lang='it'))\n","emb_hyp = torch.tensor(laser.embed_sentences(hypothesis,lang='it'))\n","\n","emb_prem_rep = torch.repeat_interleave(emb_prem,4,dim=0)\n","emb_hyp_rep = emb_hyp.repeat(1998,1)\n","\n","input_sp = prepare_input_xnli(emb_prem_rep,emb_hyp_rep)\n","output_sp = mlp_xnli(input_sp.cuda())\n","out_pos = output_sp[:][:,0]\n","out_pos_res = out_pos.reshape(1998,4)\n","\n","preds = torch.argmax(out_pos_res,dim=1)\n","labels = torch.tensor(pols).cuda()\n","n_corr = torch.sum(preds==labels)\n","\n","print(f\"Accuracy {n_corr/1998}\")"],"metadata":{"id":"wytBXjelfITj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### LASER embeddings + Multilingual Amazon Reviews MLP Training/Inference"],"metadata":{"id":"CrOF2zD9G3-c"}},{"cell_type":"markdown","source":["##### LASER embeddings + Multilingual Amazon Reviews MLP Classes and Methods"],"metadata":{"id":"tagvKkUwjHwx"}},{"cell_type":"code","source":["def compute_accuracy_mlar(model, dataloader):\n","  n_correct = 0\n","  tot = 0\n","  model.eval()\n","\n","  for batch in dataloader:\n","    inputs = batch['reviews'].to('cuda')\n","    labels = batch['labels'].to('cuda')\n","\n","    outputs = model(inputs)\n","    preds = torch.argmax(outputs, dim=1)\n","\n","    check = preds == labels\n","\n","    n_correct = n_correct + torch.sum(check)\n","    tot = tot + labels.shape[0]\n","\n","  acc = n_correct/tot\n","\n","  return acc"],"metadata":{"id":"O_Vjg4XNUwWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Turn our labels and encodings into a Dataset object\n","\n","class MLARDataset(torch.utils.data.Dataset):\n","    def __init__(self, reviews, labels):\n","        self.reviews = reviews\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        #item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item = {}\n","        item['reviews'] = torch.tensor(self.reviews[idx], dtype=torch.float32)\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"id":"qcXzc1T03_ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_mlar(model,\n","          optimizer,\n","          criterion,\n","          trainloader,\n","          validloader,\n","          device='cuda',\n","          epochs=5):\n","\n","  best_valid_loss = float('Inf')\n","  model.train()\n","\n","  for epoch in range(epochs):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","\n","        inputs = data['reviews'].to('cuda')\n","        labels = data['labels'].to('cuda')\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 200 == 199:\n","            model.eval()\n","\n","            with torch.no_grad():\n","              valid_loss = 0.0\n","              nb_batch_eval = 0\n","              for batch_eval in validloader:\n","                inputs = batch_eval['reviews'].to('cuda')\n","                labels = batch_eval['labels'].to('cuda')\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                valid_loss += loss.item()\n","                nb_batch_eval = nb_batch_eval + 1\n","\n","            print(f'[{epoch + 1}, {i + 1:5d}] train loss: {running_loss / 200:.3f}, valid loss: {valid_loss / nb_batch_eval:.3f}')\n","\n","            if best_valid_loss > valid_loss:\n","              best_valid_loss = valid_loss\n","              torch.save(model,'/content/gdrive/MyDrive/models/best_mlar.pt')\n","\n","            running_loss = 0.0\n","\n","            model.train()\n","\n","  print('Finished Training')"],"metadata":{"id":"pmzMdhVK40Sd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LASER embeddings + Multilingual Amazon Reviews MLP Training"],"metadata":{"id":"mKIaEPc8Pood"}},{"cell_type":"code","source":["# Set GPU and random seed\n","np.random.seed(158)\n","torch.manual_seed(158)\n","torch.cuda.manual_seed(158)"],"metadata":{"id":"gkBUkBtyI_hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["laser = Laser()"],"metadata":{"id":"yMfOr55xKPZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp_amazon_rev_multi = MLP(idim=1024,\n","                           odim=5,\n","                           nhid=[512, 256, 64],\n","                           activation='relu',\n","                           batch_norm = True,\n","                           dropout = 0.1,\n","                           dropout_first = False)"],"metadata":{"id":"Ytof-kl6KP7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["amazon_rev_multi_en = load_dataset(\"amazon_reviews_multi\", \"en\")\n","amazon_rev_multi_fr = load_dataset(\"amazon_reviews_multi\", \"fr\")"],"metadata":{"id":"weyjE2eRHTX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#amazon_revs_body_train_en  = laser.embed_sentences(amazon_rev_multi_en[\"train\"][\"review_body\"],lang=\"en\")\n","#amazon_revs_body_valid_fr  = laser.embed_sentences(amazon_rev_multi_fr[\"validation\"][\"review_body\"],lang=\"fr\")\n","#amazon_stars_train_en = np.array(amazon_rev_multi_en[\"train\"][\"stars\"])\n","#amazon_stars_valid_fr = np.array(amazon_rev_multi_fr[\"validation\"][\"stars\"])\n","\n","amazon_revs_body_train_en = np.loadtxt('/content/gdrive/MyDrive/Data/amazon_revs_body_train_en.csv', delimiter=',', dtype=np.float32)\n","amazon_revs_body_valid_fr = np.loadtxt('/content/gdrive/MyDrive/Data/amazon_revs_body_valid_fr.csv', delimiter=',', dtype=np.float32)\n","amazon_stars_train_en = np.loadtxt('/content/gdrive/MyDrive/Data/amazon_stars_train_en.csv', delimiter=',', dtype=np.float32)\n","amazon_stars_valid_fr = np.loadtxt('/content/gdrive/MyDrive/Data/amazon_stars_valid_fr.csv', delimiter=',', dtype=np.float32)\n"],"metadata":{"id":"pe62ZOcSJDVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#np.savetxt('/content/gdrive/MyDrive/Data/amazon_revs_body_train_en.csv', amazon_revs_body_train_en, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/amazon_revs_body_valid_fr.csv', amazon_revs_body_valid_fr, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/amazon_stars_train_en.csv', amazon_stars_train_en, delimiter=',')\n","#np.savetxt('/content/gdrive/MyDrive/Data/amazon_stars_valid_fr.csv', amazon_stars_valid_fr, delimiter=',')"],"metadata":{"id":"9DGtFIjNPLZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = MLARDataset(amazon_revs_body_train_en, amazon_stars_train_en - 1)     # stars [1..5] -> labels [0..4]\n","valid_dataset = MLARDataset(amazon_revs_body_valid_fr, amazon_stars_valid_fr - 1)     # stars [1..5] -> labels [0..4]"],"metadata":{"id":"4pdDSG5q5t9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainloader = DataLoader(train_dataset,batch_size=256,shuffle=True)\n","validloader = DataLoader(valid_dataset,batch_size=256,shuffle=True)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(mlp_amazon_rev_multi.parameters(),lr=1e-4)\n","\n","train_mlar(model=mlp_amazon_rev_multi,\n","           optimizer=optimizer,\n","           criterion = criterion,\n","           trainloader = trainloader,\n","           validloader = validloader,\n","           epochs=10)"],"metadata":{"id":"1hlblPw06InU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save model\n","torch.save(mlp_amazon_rev_multi,'/content/gdrive/MyDrive/models/last_mlar.pt')"],"metadata":{"id":"1r2UjU_c6kLo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### LASER embeddings + Multilingual Amazon Reviews MLP Inference"],"metadata":{"id":"p-peWXmtPth1"}},{"cell_type":"code","source":["laser = Laser()"],"metadata":{"id":"XxUrapmuP700"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp_amazon_rev_multi = MLP(idim=1024,\n","                           odim=5,\n","                           nhid=[512, 256, 64],\n","                           activation='relu',\n","                           batch_norm=True,\n","                           dropout = 0.1,\n","                           dropout_first = False)\n","mlp_amazon_rev_multi = torch.load('/content/gdrive/MyDrive/models/best_mlar.pt')"],"metadata":{"id":"x78KHBUXP8Kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["amazon_rev_multi_de = load_dataset(\"amazon_reviews_multi\", \"de\")\n","amazon_rev_multi_en = load_dataset(\"amazon_reviews_multi\", \"en\")\n","amazon_rev_multi_es = load_dataset(\"amazon_reviews_multi\", \"es\")\n","amazon_rev_multi_fr = load_dataset(\"amazon_reviews_multi\", \"fr\")\n","amazon_rev_multi_ja = load_dataset(\"amazon_reviews_multi\", \"ja\")\n","amazon_rev_multi_zh = load_dataset(\"amazon_reviews_multi\", \"zh\")"],"metadata":{"id":"Koth1Ez3QEbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["amazon_revs_body_test_de  = laser.embed_sentences(amazon_rev_multi_en[\"test\"][\"review_body\"],lang=\"de\")\n","amazon_revs_body_test_en  = laser.embed_sentences(amazon_rev_multi_en[\"test\"][\"review_body\"],lang=\"en\")\n","amazon_revs_body_test_es  = laser.embed_sentences(amazon_rev_multi_en[\"test\"][\"review_body\"],lang=\"es\")\n","amazon_revs_body_test_fr  = laser.embed_sentences(amazon_rev_multi_fr[\"test\"][\"review_body\"],lang=\"fr\")\n","amazon_revs_body_test_ja  = laser.embed_sentences(amazon_rev_multi_en[\"test\"][\"review_body\"],lang=\"ja\")\n","amazon_revs_body_test_zh  = laser.embed_sentences(amazon_rev_multi_fr[\"test\"][\"review_body\"],lang=\"zh\")\n","\n","amazon_stars_test_de = np.array(amazon_rev_multi_de[\"test\"][\"stars\"])\n","amazon_stars_test_en = np.array(amazon_rev_multi_en[\"test\"][\"stars\"])\n","amazon_stars_test_es = np.array(amazon_rev_multi_es[\"test\"][\"stars\"])\n","amazon_stars_test_fr = np.array(amazon_rev_multi_fr[\"test\"][\"stars\"])\n","amazon_stars_test_ja = np.array(amazon_rev_multi_ja[\"test\"][\"stars\"])\n","amazon_stars_test_zh = np.array(amazon_rev_multi_zh[\"test\"][\"stars\"])"],"metadata":{"id":"Em49faJXQzuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset_de = MLARDataset(amazon_revs_body_test_de, amazon_stars_test_de - 1)     # stars [1..5] -> labels [0..4]\n","test_dataset_en = MLARDataset(amazon_revs_body_test_en, amazon_stars_test_en - 1)     # stars [1..5] -> labels [0..4]\n","test_dataset_es = MLARDataset(amazon_revs_body_test_es, amazon_stars_test_es - 1)     # stars [1..5] -> labels [0..4]\n","test_dataset_fr = MLARDataset(amazon_revs_body_test_fr, amazon_stars_test_fr - 1)     # stars [1..5] -> labels [0..4]\n","test_dataset_ja = MLARDataset(amazon_revs_body_test_ja, amazon_stars_test_ja - 1)     # stars [1..5] -> labels [0..4]\n","test_dataset_zh = MLARDataset(amazon_revs_body_test_zh, amazon_stars_test_zh - 1)     # stars [1..5] -> labels [0..4]"],"metadata":{"id":"r66vpkniRpHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testloader_de = DataLoader(test_dataset_de,batch_size=256,shuffle=True)\n","testloader_en = DataLoader(test_dataset_en,batch_size=256,shuffle=True)\n","testloader_es = DataLoader(test_dataset_es,batch_size=256,shuffle=True)\n","testloader_fr = DataLoader(test_dataset_fr,batch_size=256,shuffle=True)\n","testloader_ja = DataLoader(test_dataset_ja,batch_size=256,shuffle=True)\n","testloader_zh = DataLoader(test_dataset_zh,batch_size=256,shuffle=True)"],"metadata":{"id":"AduDDk2oTc8h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["compute_accs = lambda dl : compute_accuracy_mlar(mlp_amazon_rev_multi,dl)"],"metadata":{"id":"d8telD9GT9wq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dls = [testloader_de,\n","       testloader_en,\n","       testloader_es,\n","       testloader_fr,\n","       testloader_ja,\n","       testloader_zh]\n","\n","accs = map(compute_accs,dls)"],"metadata":{"id":"0T_e9kwlUEey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(accs)"],"metadata":{"id":"g4KphUxrYpLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df_sp = pd.read_csv(r'/content/gdrive/MyDrive/sentipolc_test_set_preprocessed.csv', skiprows=1, names=[\"idtwitter\",\"subj\",\"opos\",\"oneg\",\"iro\",\"lpos\",\"lneg\",\"top\",\"text\",\"polarity\"])\n","\n","sequences = test_df_sp['text'].tolist()\n","pols = test_df_sp['polarity'].tolist()\n","\n","input_sp = torch.tensor(laser.embed_sentences(sequences,lang='it'))\n","output_sp = mlp_amazon_rev_multi(input_sp.cuda())\n","\n","preds = torch.argmax(output_sp,dim=1)\n","labels = torch.tensor(pols).cuda()\n","n_corr = torch.sum(preds==labels)\n","\n","print(f\"Accuracy: {n_corr/1998}\")"],"metadata":{"id":"lMrKQMNzaDuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LASER Language-Agnostic SEntence Representations"],"metadata":{"id":"aSi5cJ7SR098"}},{"cell_type":"code","source":["print(\"LASER Language-Agnostic SEntence Representations\")"],"metadata":{"id":"TM-4j8S6czNp"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["FP1zwA6HlBWg","h5EJCCHWAodG","PYdk4zWNe-D0","cAo-ZZc4cl_A","YoIsNcs5uiDg","C2FmktP78Gar","rQe_2xU7FqjV","E6cZh8fPi4Sc","2RQHV3SEe3I_","bxVBTQKTe9nh","CrOF2zD9G3-c","tagvKkUwjHwx","mKIaEPc8Pood","p-peWXmtPth1","aSi5cJ7SR098"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}