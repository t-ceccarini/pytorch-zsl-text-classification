{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["YPISafRuZUK3","RLWceP3n3eLT","ssKH6o0e3r2y","PHtGPyts5fKM","38dUQJn75ICC","GlAkn84q47h_"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Sentiment Analysis on Italian Tweets\n","In this tutorial we'll be building a machine learning model for the sentiment analysis of italian tweets. Further details on the sentipolc dataset used can be found [here](http://www.di.unito.it/~tutreeb/sentipolc-evalita16/sentipolc-guidelines2016UPDATED130916.pdf). We'll focus only on the polarity classification task."],"metadata":{"id":"Zx21VkJAC9Pq"}},{"cell_type":"markdown","metadata":{"id":"Lf22crFfYvOA"},"source":["Upload the datasets on Google Drive and execute the next cell.\n"]},{"cell_type":"code","metadata":{"id":"1wtfzgDPYTWC"},"source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPISafRuZUK3"},"source":["## Install dependencies and import libraries"]},{"cell_type":"code","metadata":{"id":"PKbzH9zYZTD2"},"source":["# Transformers installation\n","! pip install transformers datasets --quiet\n","# To install from source instead of the last release, comment the command above and uncomment the following one.\n","# ! pip install git+https://github.com/huggingface/transformers.git\n","\n","# Ekphrasis installation for datasets preprocessing\n","# ! pip install ekphrasis --quiet\n","! pip install git+https://github.com/fucaja/ekphrasis.git --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lblrObbP6KKg"},"source":["# Import libraries\n","\n","# Preprocessing the datasets\n","import pandas as pd\n","import numpy as np\n","import torch\n","import os\n","import re\n","from ekphrasis.classes.preprocessor import TextPreProcessor\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","from ekphrasis.classes.segmenter import Segmenter\n","\n","# Define the model\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n","import torch.nn as nn\n","\n","# Pre-training function with Pytorch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Fine-tuning with Trainer\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_metric\n","\n","# Zip and download results\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFJfWMe-E_Vp"},"source":["# Clone ekphrasis repo\n","!git clone https://github.com/cbaziotis/ekphrasis.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLWceP3n3eLT"},"source":["## Preprocessing the datasets"]},{"cell_type":"code","metadata":{"id":"1t9Py2HJZip-"},"source":["train_df = pd.read_csv(r'/content/gdrive/MyDrive/training_set_sentipolc16.csv')\n","#train_df = pd.read_csv(r'/content/eda_train_data.csv', sep='\\t', names=[\"polarity\",\"text\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ex22WL5OTmDc"},"source":["#test_df = pd.read_csv(r'/content/gdrive/MyDrive/test_set_sentipolc16_gold2000.csv', sep='delimiter', engine='python', names=[\"idtwitter\",\"subj\",\"opos\",\"oneg\",\"iro\",\"lpos\",\"lneg\",\"top\",\"text\"])\n","test_df = pd.read_csv(r'/content/gdrive/MyDrive/test_set_sentipolc16_gold2000.csv', error_bad_lines=False, names=[\"idtwitter\",\"subj\",\"opos\",\"oneg\",\"iro\",\"lpos\",\"lneg\",\"top\",\"text\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jdwy8HfhkLQF"},"source":["In order to train the model, we'll create the column 'polarity' based on the two columns 'opos' and 'oneg' as follows:\n","\n","| opos | oneg | polarity | label    |\n","|------|------|----------|----------|\n","|   1  |   0  |     0    | Positive |\n","|   0  |   1  |     1    | Negative |\n","|   1  |   1  |     2    | Mixed    |\n","|   0  |   0  |     3    | Neutral  |\n"]},{"cell_type":"code","metadata":{"id":"uq1qxsqBj-_X"},"source":["# Create a list of conditions\n","def create_conditions(df):\n","    conditions = [\n","    (df['opos'] == 1) & (df['oneg'] == 0),\n","    (df['opos'] == 0) & (df['oneg'] == 1),\n","    (df['opos'] == 1) & (df['oneg'] == 1),\n","    (df['opos'] == 0) & (df['oneg'] == 0)\n","    ]\n","    return conditions\n","\n","# Create a list of the values we want to assign for each condition\n","polarities = [0, 1, 2, 3]\n","\n","# Create column polarity and use np.select to assign values to it using our lists as arguments\n","train_df['polarity'] = np.select(create_conditions(train_df), polarities)\n","test_df['polarity'] = np.select(create_conditions(test_df), polarities)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZERnogAJ7CDX"},"source":["# Make text lowercase\n","#train_df['text'] = train_df['text'].str.lower()\n","#test_df['text'] = test_df['text'].str.lower()\n","\n","#train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\n","#test_df['text'] = test_df['text'].str.replace('[^\\w\\s]','')\n","\n","#train_df['text'] = train_df['text'].str.replace(',','')\n","#test_df['text'] = test_df['text'].str.replace(',','')\n","\n","#train_df['text'] = train_df['text'].str.replace('.','')\n","#test_df['text'] = test_df['text'].str.replace('.','')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjvwWM3u4thf"},"source":["# Select only positive and negative polarity (use num_labels=2 in this case)\n","#train_df = train_df.loc[(train_df['polarity'] == 0) | (train_df['polarity'] == 1)]\n","#test_df = test_df.loc[(test_df['polarity'] == 0) | (test_df['polarity'] == 1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfpNP-NinUHl"},"source":["# Display DataFrame with the new column\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JKq0Eo5pZINH"},"source":["test_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKtA4i770zCe"},"source":["# Export text for statistics generation\n","try:\n","    os.mkdir('texts')\n","except OSError:\n","    print (\"Creation of the directory failed\")\n","\n","np.savetxt(r'texts/train_texts.txt', train_df[\"text\"].values, fmt='%s')\n","np.savetxt(r'texts/test_texts.txt', test_df[\"text\"].values, fmt='%s')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1hP-okE2QUX"},"source":["# Generate word statistics\n","! python /content/ekphrasis/ekphrasis/tools/generate_stats.py --input /content/texts/ --name sentipolc16 --ngrams 2 --mincount 70 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHHN0U3Hnl3y"},"source":["# Create lists with text and polarity columns\n","train_texts = train_df[\"text\"].tolist()\n","train_labels = train_df[\"polarity\"].tolist()\n","\n","test_texts = test_df[\"text\"].tolist()\n","test_labels = test_df[\"polarity\"].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwRf_Ale_jtP"},"source":["# Define a preprocessing pipeline with ekphrasis\n","\n","text_processor = TextPreProcessor(\n","    # terms that will be normalized\n","    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n","        'time', 'date', 'number'],\n","    # terms that will be annotated\n","    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\", 'emphasis', 'censored'},\n","    annotate={\"hashtag\"},\n","    fix_html=True,  # fix HTML tokens\n","\n","    # corpus from which the word statistics are going to be used\n","    # for word segmentation\n","    #segmenter=\"twitter\",\n","    #segmenter = Segmenter(corpus=\"sentipolc16\"),\n","\n","    # corpus from which the word statistics are going to be used\n","    # for spell correction\n","    #corrector=\"twitter\",\n","\n","    unpack_hashtags=True,  # perform word segmentation on hashtags\n","    #unpack_contractions=True,  # Unpack contractions (can't -> can not)\n","    #spell_correct_elong=False,  # spell correction for elongated words\n","\n","    # select a tokenizer. You can use SocialTokenizer, or pass your own\n","    # the tokenizer, should take as input a string and return a list of tokens\n","    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n","\n","    # list of dictionaries, for replacing tokens extracted from the text,\n","    # with other expressions. You can pass more than one dictionaries.\n","    dicts=[emoticons]\n",")\n","\n","def preprocess(text, do_lower_case=True):\n","    if do_lower_case:\n","        text = text.lower()\n","    text = str(\" \".join(text_processor.pre_process_doc(text)))\n","    text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n","    text = re.sub(r'^\\s', '', text)\n","    text = re.sub(r'\\s$', '', text)\n","\n","    return text\n","\n","clean_train_texts = []\n","clean_test_texts = []\n","\n","for text in train_texts:\n","    #print(\" \".join(text_processor.pre_process_doc(text)))\n","    #clean_train_texts.append(\" \".join(text_processor.pre_process_doc(text)))\n","    clean_train_texts.append(preprocess(text))\n","\n","for text in test_texts:\n","    #print(\" \".join(text_processor.pre_process_doc(text)))\n","    #clean_test_texts.append(\" \".join(text_processor.pre_process_doc(text)))\n","    clean_test_texts.append(preprocess(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9aiyVGccWJ4"},"source":["train_texts[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYFMtfk7bJgI"},"source":["clean_train_texts[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bicrFeRww3tz"},"source":["train_texts = clean_train_texts\n","test_texts = clean_test_texts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save the processed version of the datasets (useful for other operations, optional)."],"metadata":{"id":"wy5ulXsYxW0D"}},{"cell_type":"code","source":["train_df[\"text\"] = train_texts\n","test_df[\"text\"] = test_texts"],"metadata":{"id":"uskWkfypxkYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_df.to_csv('sentipolc_train_set_preprocessed.csv', index=False)\n","#!cp -r '/content/sentipolc_train_set_preprocessed.csv' /content/gdrive/MyDrive/\n","\n","test_df.to_csv('sentipolc_test_set_preprocessed.csv', index=False)\n","!cp -r '/content/sentipolc_test_set_preprocessed.csv' /content/gdrive/MyDrive/"],"metadata":{"id":"IWzuVXnSx1a2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssKH6o0e3r2y"},"source":["## Define the model"]},{"cell_type":"code","metadata":{"id":"XHMdFNdA1DSG"},"source":["# Set random seed and set device to GPU.\n","torch.manual_seed(0)\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","else:\n","    device = torch.device('cpu')\n","\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzSVz6ktpve7"},"source":["Create tokenizer and pretrained AlBERTo model. For further details on AlBERTo see [here](https://github.com/marcopoli/AlBERTo-it)."]},{"cell_type":"code","metadata":{"id":"u6_ek_kwox2d"},"source":["# Create tokenizer and pretrained umberto model\n","#tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n","#model = AutoModelForSequenceClassification.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\", num_labels = 4)\n","\n","# Create tokenizer and pretrained alberto model\n","tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\", num_labels=4)\n","\n","#model = AutoModelForSequenceClassification.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\", num_labels = 2)\n","\n","#tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-wikipedia-uncased-v1\")\n","#model = AutoModelForSequenceClassification.from_pretrained(\"Musixmatch/umberto-wikipedia-uncased-v1\",\n","#                                                           num_labels = 4,\n","                                                           #attention_probs_dropout_prob=0.2,\n","                                                           #hidden_dropout_prob=0.4\n","#                                                           )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this cell we create a custom model based on AlBERTo, skip this cell if you want to perform fine tuning on the base AlBERTo model"],"metadata":{"id":"k4QwtsOgL306"}},{"cell_type":"code","metadata":{"id":"Gzoyv63tP65Z"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","\n","class SAModel(nn.Module):\n","    def __init__(self, dropout_rate=0.1, num_labels=4):\n","        super(SAModel, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","        self.linear1 = nn.Linear(768, 384)\n","        self.ln1 = nn.LayerNorm(384)\n","\n","        self.dropout2 = nn.Dropout(dropout_rate)\n","        self.linear2 = nn.Linear(384, 64)\n","        self.ln2 = nn.LayerNorm(64)\n","\n","        self.dropout3 = nn.Dropout(dropout_rate)\n","        self.linear3 = nn.Linear(64, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","\n","        outputs = self.dropout1(outputs[0][:,0,:].view(-1,768))\n","        outputs = self.linear1(outputs)\n","        outputs = self.ln1(outputs)\n","        outputs = torch.nn.Tanh()(outputs)\n","\n","        outputs = self.dropout2(outputs)\n","        outputs = self.linear2(outputs)\n","        outputs = self.ln2(outputs)\n","        outputs = torch.nn.Tanh()(outputs)\n","\n","        outputs = self.dropout3(outputs)\n","        outputs = self.linear3(outputs)\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBHv_lIHtcbE"},"source":["# Only for the custom model\n","model = SAModel().to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTBpmAMvqWbB"},"source":["# Tokenize texts\n","train_encodings = tokenizer(train_texts, padding=True)\n","test_encodings = tokenizer(test_texts, padding=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_encodings['attention_mask']"],"metadata":{"id":"IWShwzIfZc6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKvSE4gotVQ5"},"source":["# Turn our labels and encodings into a Dataset object\n","\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TextDataset(train_encodings, train_labels)\n","test_dataset = TextDataset(test_encodings, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EYrBxV8akNC"},"source":["# Freeze some layers (optional)\n","freeze_layers = \"5,6,7,8,9,10,11\"\n","\n","if freeze_layers is not \"\":\n","        layer_indexes = [int(x) for x in freeze_layers.split(\",\")]\n","        for layer_idx in layer_indexes:\n","             for param in list(model.roberta.encoder.layer[layer_idx].parameters()):\n","                 param.requires_grad = False\n","             print (\"Froze Layer: \", layer_idx)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHtGPyts5fKM"},"source":["## Fine-tuning in native PyTorch"]},{"cell_type":"code","metadata":{"id":"9JYWlYeRWolA"},"source":["# Functions for saving and loading model parameters and metrics.\n","def save_checkpoint(path, model, valid_loss):\n","    torch.save({'model_state_dict': model.state_dict(),\n","                  'valid_loss': valid_loss}, path)\n","\n","\n","def load_checkpoint(path, model):\n","    state_dict = torch.load(path, map_location=device)\n","    model.load_state_dict(state_dict['model_state_dict'])\n","\n","    return state_dict['valid_loss']\n","\n","\n","def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):\n","    state_dict = {'train_loss_list': train_loss_list,\n","                  'valid_loss_list': valid_loss_list,\n","                  'global_steps_list': global_steps_list}\n","\n","    torch.save(state_dict, path)\n","\n","\n","def load_metrics(path):\n","    state_dict = torch.load(path, map_location=device)\n","    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYwHMWE-k4SW"},"source":["# Pre-training function with Pytorch\n","\n","def pretrain(model,\n","            optimizer,\n","            train_loader,\n","            valid_loader,\n","            num_epochs,\n","            output_path,\n","            valid_period,\n","            scheduler=None):\n","\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    # Pretrain linear layers, do not train bert\n","    #for param in model.roberta.parameters():\n","    for param in model.bert.parameters():\n","        param.requires_grad = False\n","\n","    model.train()\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    global_step = 0\n","\n","    for epoch in range(num_epochs):\n","        for batch_train in train_loader:\n","\n","            optim.zero_grad()\n","\n","            input_ids = batch_train['input_ids'].to(device)\n","            attention_mask = batch_train['attention_mask'].to(device)\n","            labels = batch_train['labels'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","\n","            loss = nn.CrossEntropyLoss()(outputs,labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            train_loss += loss.item()\n","            global_step += 1\n","\n","            if global_step % valid_period == 0:\n","                model.eval()\n","\n","                with torch.no_grad():\n","                    for batch_eval in valid_loader:\n","\n","                        input_ids = batch_eval['input_ids'].to(device)\n","                        attention_mask = batch_eval['attention_mask'].to(device)\n","                        labels = batch_eval['labels'].to(device)\n","\n","                        outputs = model(input_ids, attention_mask=attention_mask)\n","\n","                        loss = nn.CrossEntropyLoss()(outputs,labels)\n","\n","                        valid_loss += loss.item()\n","\n","                train_loss = train_loss / valid_period\n","                valid_loss = valid_loss / len(valid_loader)\n","\n","                model.train()\n","\n","                # print summary\n","                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n","                      .format(epoch+1, num_epochs, global_step, num_epochs*valid_period,\n","                              train_loss, valid_loss))\n","\n","                train_loss = 0.0\n","                valid_loss = 0.0\n","\n","    # Set bert parameters back to trainable\n","    #for param in model.roberta.parameters():\n","    for param in model.bert.parameters():\n","        param.requires_grad = True\n","\n","    print('Pre-training done!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVgkn-6pWi1B"},"source":["# Training function with Pytorch\n","\n","def train(model,\n","          optimizer,\n","          train_loader,\n","          valid_loader,\n","          num_epochs,\n","          output_path,\n","          valid_period,\n","          scheduler=None):\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    train_loss_list = []\n","    valid_loss_list = []\n","    best_valid_loss = float('Inf')\n","\n","    global_step = 0\n","    global_steps_list = []\n","\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        for batch_train in train_loader:\n","\n","            optim.zero_grad()\n","\n","            input_ids = batch_train['input_ids'].to(device)\n","            attention_mask = batch_train['attention_mask'].to(device)\n","            labels = batch_train['labels'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","\n","            loss = nn.CrossEntropyLoss()(outputs,labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            train_loss += loss.item()\n","            global_step += 1\n","\n","            if global_step % valid_period == 0:\n","                model.eval()\n","\n","                with torch.no_grad():\n","                    for batch_eval in valid_loader:\n","                        input_ids = batch_eval['input_ids'].to(device)\n","                        attention_mask = batch_eval['attention_mask'].to(device)\n","                        labels = batch_eval['labels'].to(device)\n","\n","                        outputs = model(input_ids, attention_mask=attention_mask)\n","\n","                        loss = nn.CrossEntropyLoss()(outputs,labels)\n","\n","                        valid_loss += loss.item()\n","\n","                train_loss = train_loss / valid_period\n","                valid_loss = valid_loss / len(valid_loader)\n","                train_loss_list.append(train_loss)\n","                valid_loss_list.append(valid_loss)\n","                global_steps_list.append(global_step)\n","\n","                # print summary\n","                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n","                      .format(epoch+1, num_epochs, global_step, num_epochs*valid_period,\n","                              train_loss, valid_loss))\n","\n","                # checkpoint\n","                if best_valid_loss > valid_loss:\n","                    best_valid_loss = valid_loss\n","                    save_checkpoint(output_path + '/model.pt', model, best_valid_loss)\n","                    save_metrics(output_path + '/metric.pt', train_loss_list, valid_loss_list, global_steps_list)\n","\n","                train_loss = 0.0\n","                valid_loss = 0.0\n","                model.train()\n","\n","    save_metrics(output_path + '/metric.pt', train_loss_list, valid_loss_list, global_steps_list)\n","    print('Training done!')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jymux4bXpt7n"},"source":["model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PY4c_N6X5XJ"},"source":["# Fine-tuning with Pytorch\n","\n","output_path = '/content'\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n","\n","print(\"Start pretraining\")\n","\n","num_epochs = 3\n","\n","optim = AdamW(model.parameters(), lr=1e-4)\n","scheduler = get_linear_schedule_with_warmup(optim,\n","                                            num_warmup_steps=len(train_loader)*1,\n","                                            num_training_steps=len(train_loader)*num_epochs)\n","\n","pretrain(model=model,\n","         optimizer=optim,\n","         train_loader=train_loader,\n","         valid_loader=valid_loader,\n","         num_epochs=num_epochs,\n","         output_path=output_path,\n","         valid_period=len(train_loader),\n","         scheduler=scheduler\n","         )\n","\n","print(\"Start training\")\n","\n","num_epochs = 3\n","\n","optim = AdamW(model.parameters(), lr=2e-5)\n","scheduler = get_linear_schedule_with_warmup(optim,\n","                                            num_warmup_steps=len(train_loader)*2,\n","                                            num_training_steps=len(train_loader)*num_epochs)\n","\n","train(model=model,\n","      optimizer=optim,\n","      train_loader=train_loader,\n","      valid_loader=valid_loader,\n","      num_epochs=num_epochs,\n","      output_path=output_path,\n","      valid_period=len(train_loader),\n","      scheduler=scheduler\n","      )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWWN7AvPS1py"},"source":["# Load best model\n","device = torch.device('cuda:0')\n","load_checkpoint(output_path + '/model.pt', model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ao5mss3svSRy"},"source":["# Evaluate model with Pytorch\n","\n","y_pred = []\n","y_true = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for batch_eval in valid_loader:\n","        input_ids = batch_eval['input_ids'].to(device)\n","        attention_mask = batch_eval['attention_mask'].to(device)\n","        labels = batch_eval['labels'].to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","\n","        y_pred.extend(torch.argmax(outputs, axis=-1).tolist())\n","        #y_pred.extend(torch.argmax(outputs[0], axis=-1).tolist())\n","        y_true.extend(labels.tolist())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKSL9H6dyfSO"},"source":["print(classification_report(y_true, y_pred, labels=[0, 1, 2, 3]))\n","\n","cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n","ax = plt.subplot()\n","\n","sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n","\n","ax.set_title('Confusion Matrix')\n","\n","ax.set_xlabel('Predicted Labels')\n","ax.set_ylabel('True Labels')\n","\n","ax.xaxis.set_ticklabels(['Positive', 'Negative', 'Mixed', 'Neutral'])\n","ax.yaxis.set_ticklabels(['Positive', 'Negative', 'Mixed', 'Neutral'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92ih1CLz1Usr"},"source":["# Save fine-tuned model\n","torch.save(model,'/content/model_6_pre_1_ep_32_bs_4_nc.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"38dUQJn75ICC"},"source":["## Fine-tuning in PyTorch with the Trainer API"]},{"cell_type":"code","metadata":{"id":"kcX40w1WrTwy"},"source":["# Fine-tuning with Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,             # total number of training epochs\n","    per_device_train_batch_size=64,  # batch size per device during training\n","    per_device_eval_batch_size=16,   # batch size for evaluation\n","    learning_rate=2e-5,             # the initial learning rate for AdamW optimizer\n","    #max_grad_norm=0.01,             # maximum gradient norm (for gradient clipping)\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.1,                # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=25,\n","    save_strategy='epoch',            # save is done at the end of each epoch\n","    evaluation_strategy='epoch',\n","    eval_steps='epoch',              # evaluation is done at the end of each epoch\n","    load_best_model_at_end=True      # whether or not to load the best model found during training at the end of training\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=test_dataset            # evaluation dataset\n",")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylT4NOW8SA0J"},"source":["# start training for fine-tuning with Trainer\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1XptoZG73HT"},"source":["# Compute metrics\n","\n","acc = load_metric(\"accuracy\")\n","f1 = load_metric(\"f1\")\n","\n","def compute_acc(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return acc.compute(predictions=predictions, references=labels)\n","\n","def compute_f1(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return f1.compute(predictions=predictions, references=labels, average='macro')\n","\n","trainer_acc = Trainer(\n","    model=model,\n","    args=training_args,\n","    #train_dataset=train_dataset,\n","    train_dataset=test_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_acc,\n",")\n","\n","trainer_f1 = Trainer(\n","    model=model,\n","    args=training_args,\n","    #train_dataset=train_dataset,\n","    train_dataset=test_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_f1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sIDJbxNqOYC9"},"source":["trainer_acc.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LF9Z-XmaOTyF"},"source":["trainer_f1.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GlAkn84q47h_"},"source":["## Zip and download fine-tuned model"]},{"cell_type":"code","metadata":{"id":"80lS4Q3W1ck0"},"source":["# Zip and download results folder\n","\n","#Fine-tuning in native PyTorch\n","!zip -r /content/model.zip /content/model.pt\n","!cp -r '/content/model.zip' /content/gdrive/MyDrive/\n","\n","#Fine-tuning with the Trainer API\n","#!zip -r /content/results.zip /content/results\n","#!cp -r '/content/results.zip' /content/gdrive/MyDrive/\n"],"execution_count":null,"outputs":[]}]}